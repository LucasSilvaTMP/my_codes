{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "365dcc56-45a7-4c5b-b8d9-912f05f18d4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataeng_services.session import Session\n",
    "\n",
    "session_provider = Session().client(\"session-provider\")\n",
    "aws_auth_dict = session_provider.get_temporary_credentials().json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "225a07da-c828-45df-9960-c5da644b4daa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Módulo necessário para \"encontrar\" o Spark no ambiente\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Módulo do PySpark\n",
    "import pyspark\n",
    "\n",
    "# Classes necessárias para configurar e iniciar o Spark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cffc9fa3-feee-4484-8435-5a4d231a3b27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7faa265513d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definindo o nome que daremos ao seu job Spark\n",
    "name = \"sohtec_database_query\" \n",
    "\n",
    "# Criando o objeto com as configurações do Spark e atribuindo o nome ao job\n",
    "conf = SparkConf().setAppName(name)\n",
    "\n",
    "# Adicionando as configurações do Spark\n",
    "\n",
    "# Aqui definiremos até quantos cores (cpus) cada executor terá a sua disposição para executar sua query\n",
    "conf.set(\"spark.kubernetes.executor.limit.cores\", 2 )\n",
    "\n",
    "# Aqui definiremos até quanto de memória cada executor terá a sua disposição para executar sua query\n",
    "conf.set(\"spark.kubernetes.executor.limit.memory\", \"4Gi\")\n",
    "\n",
    "# Aqui é definido o request cores, valor mínimo que o Kubernetes garante de ciclos de máquina para cada CPU.\n",
    "conf.set(\"spark.kubernetes.executor.request.cores\", 1 )\n",
    "\n",
    "# Aqui é definido a memória que cada executor do Spark possuirá para executar o job\n",
    "conf.set(\"spark.kubernetes.executor.request.memory\", \"2G\" )\n",
    "\n",
    "# Aqui é definido o número de executores (workers) que o Spark possuirá para executar o job\n",
    "conf.set(\"spark.executor.instances\", 2)\n",
    "\n",
    "# Aqui é definido o número de cores (cpus) para cada executor que o Spark possuirá para executar o job\n",
    "conf.set(\"spark.executor.cores\", 1)\n",
    "\n",
    "# Aqui é definido a memória que cada executor do Spark possuirá para executar o job\n",
    "conf.set(\"spark.executor.memory\", \"2g\")\n",
    "\n",
    "# Aqui nós passamos as credenciais retornadas do SessionProvider para o Spark\n",
    "# Isso dará ao Spark as mesmas permissões do nosso usuário de acesso aos Data-Lake.\n",
    "conf.set(\"spark.hadoop.fs.s3a.access.key\", aws_auth_dict['AccessKeyId'])\n",
    "conf.set(\"spark.hadoop.fs.s3a.secret.key\", aws_auth_dict['SecretAccessKey'])\n",
    "conf.set(\"spark.hadoop.fs.s3a.session.token\", aws_auth_dict['SessionToken'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bc18f90-4914-450c-a7ef-09efa57aa4f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7faa265513d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CONFIG DO SPARK PARA DRIVER MICROSOFT\n",
    "conf.set('spark.jars.packages', 'com.microsoft.azure:spark-mssql-connector_2.12:1.1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "497554fb-05d1-4344-bb85-eefeccda6a34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "com.microsoft.azure#spark-mssql-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-31df146a-ffac-4a83-864c-dd74108b66fa;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.microsoft.azure#spark-mssql-connector_2.12;1.1.0 in central\n",
      "downloading https://repo1.maven.org/maven2/com/microsoft/azure/spark-mssql-connector_2.12/1.1.0/spark-mssql-connector_2.12-1.1.0.jar ...\n",
      "\t[SUCCESSFUL ] com.microsoft.azure#spark-mssql-connector_2.12;1.1.0!spark-mssql-connector_2.12.jar (13ms)\n",
      ":: resolution report :: resolve 622ms :: artifacts dl 15ms\n",
      "\t:: modules in use:\n",
      "\tcom.microsoft.azure#spark-mssql-connector_2.12;1.1.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   1   |   1   |   0   ||   1   |   1   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-31df146a-ffac-4a83-864c-dd74108b66fa\n",
      "\tconfs: [default]\n",
      "\t1 artifacts copied, 0 already retrieved (76kB/6ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Aqui nós efetivamente criaremos os executores do Spark e teremos um Sessão disponível para utilizá-lo\n",
    "spark = SparkSession.builder.config(conf=conf).enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27bf5321-9a53-4f16-98b3-decc2d5d6923",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---------+----+---+--------------+----------+\n",
      "|EMPRESAID|        NOME_EMPRESA|PRETENSAO| Ano|Mes|        Origem|TotalLeads|\n",
      "+---------+--------------------+---------+----+---+--------------+----------+\n",
      "|        4|Barcellos Assesso...|  LOCACAO|2023|  7|      VivaReal|        76|\n",
      "|        4|Barcellos Assesso...|  LOCACAO|2023|  7|  WHATSAPP_API|        15|\n",
      "|        4|Barcellos Assesso...|   VENDAS|2023|  8|          SITE|         2|\n",
      "|        4|Barcellos Assesso...|   VENDAS|2023|  8|           ZAP|         9|\n",
      "|       11|IMOBILIARIA COMER...|  LOCACAO|2023|  8|          SITE|        15|\n",
      "|       11|IMOBILIARIA COMER...|  LOCACAO|2023|  8|           ZAP|       141|\n",
      "|       11|IMOBILIARIA COMER...|   VENDAS|2023|  7|           ZAP|         1|\n",
      "|       12|     Cancian Imóveis|  LOCACAO|2023|  7|           OLX|        21|\n",
      "|       16|   Banco Imobiliário|  LOCACAO|2023|  7|          SITE|        14|\n",
      "|       16|   Banco Imobiliário|  LOCACAO|2023|  7|           ZAP|        42|\n",
      "|       16|   Banco Imobiliário|  LOCACAO|2023|  8|      VivaReal|        19|\n",
      "|       16|   Banco Imobiliário|   VENDAS|2023|  7|      VivaReal|         3|\n",
      "|       19|     Bom Fim Imóveis|  LOCACAO|2023|  7|     DreamCasa|         3|\n",
      "|       19|     Bom Fim Imóveis|   VENDAS|2023|  8| CHAVES_NA_MAO|        31|\n",
      "|       19|     Bom Fim Imóveis|   VENDAS|2023|  8|          SITE|         2|\n",
      "|       19|     Bom Fim Imóveis|   VENDAS|2023|  8|           ZAP|        10|\n",
      "|       21|               Certa|  LOCACAO|2023|  7|      VivaReal|        58|\n",
      "|       22|SOMMA ALUGUÉIS & ...|  LOCACAO|2023|  8|           OLX|         3|\n",
      "|       23|    Sperinde Imóveis|  LOCACAO|2023|  7|ADMIN_PROPOSTA|         6|\n",
      "|       23|    Sperinde Imóveis|  LOCACAO|2023|  7|      VivaReal|       180|\n",
      "+---------+--------------------+---------+----+---+--------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#NICOLLE: ali embaixo temos a query feita no banco de dados de sohtec; A unica coisa que mudaremos nessa \n",
    "#query ;futuramente é a data de execução.Por hora, não precisa mexer, talvez apenas colocar como ultima \n",
    "#data 2023-10-01,já que fechamos o mês 10. \n",
    "\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:sqlserver://sohtec-db-server.database.windows.net\") \\\n",
    "    .option(\"databaseName\", \"sohtec\") \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .option(\"user\", \"sohtecleitura\") \\\n",
    "    .option(\"password\", \"Pqrxum99\") \\\n",
    "    .option(\"numPartitions\", 1) \\\n",
    "    .option(\"query\", '''\n",
    "select E.Id as EMPRESAID, E.Nome as NOME_EMPRESA, CE.Modulo as PRETENSAO, Year(CE.DataCriacao) as Ano, Month(CE.DataCriacao) as Mes, Isnull( Origem, 'SITE') as Origem, Count(distinct CE.ClienteEmail) as TotalLeads\n",
    "from ClienteEmpresa CE\n",
    "join Empresas E on E.Id = CE.EmpresaId\n",
    "where CE.Removido = 0\n",
    "and CE.EmpresaID in (select ID from Empresas where Ativo = 1) -- imobs ZAP+\n",
    "and CE.DataCriacao between '2023-07-01' and '2023-09-01'\n",
    "and CE.EmpresaId not in (1)\n",
    "group by E.Id , E.Nome, Year(CE.DataCriacao) , CE.Modulo, Month(CE.DataCriacao), Origem\n",
    "''') \\\n",
    "    .load()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8acf48b1-78c3-47ac-95f6-774605cc7897",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#NICOLLE: Aqui convertemos o dataframe pro formato PANDAS, que é o adequado pra gente\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#esse df_p é o produto final desse código inteiro, a tabela de dados sohtec. Esse df_p será o input\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df_p\u001b[38;5;241m=\u001b[39m\u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mtoPandas()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#NICOLLE: Aqui convertemos o dataframe pro formato PANDAS, que é o adequado pra gente\n",
    "#esse df_p é o produto final desse código inteiro, a tabela de dados sohtec. Esse df_p será o input\n",
    "#para a parte do outro código, em que jogamos os dados pro datalake\n",
    "df_p=df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25598cae-1c6b-4106-8e40-45b78c909838",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      EMPRESAID                      NOME_EMPRESA PRETENSAO   Ano  Mes  \\\n",
      "0             4  Barcellos Assessoria Imobiliária   LOCACAO  2023    7   \n",
      "1             4  Barcellos Assessoria Imobiliária   LOCACAO  2023    7   \n",
      "2             4  Barcellos Assessoria Imobiliária    VENDAS  2023    8   \n",
      "3             4  Barcellos Assessoria Imobiliária    VENDAS  2023    8   \n",
      "4            11             IMOBILIARIA COMERLATO   LOCACAO  2023    8   \n",
      "...         ...                               ...       ...   ...  ...   \n",
      "6272       1037         Accioly Group Imobiliária    VENDAS  2023    8   \n",
      "6273       1043                      Remax Mediar   LOCACAO  2023    8   \n",
      "6274       1043                      Remax Mediar    VENDAS  2023    8   \n",
      "6275       1044                         Lopes LRT    VENDAS  2023    8   \n",
      "6276       1044                         Lopes LRT    VENDAS  2023    8   \n",
      "\n",
      "            Origem  TotalLeads  \n",
      "0         VivaReal          76  \n",
      "1     WHATSAPP_API          15  \n",
      "2             SITE           2  \n",
      "3              ZAP           9  \n",
      "4             SITE          15  \n",
      "...            ...         ...  \n",
      "6272           ZAP          12  \n",
      "6273           ZAP          32  \n",
      "6274      VivaReal           8  \n",
      "6275           OLX           2  \n",
      "6276           ZAP          16  \n",
      "\n",
      "[6277 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1fb20a6e-f9c3-463a-ae7a-48bebcb67043",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#NICOLLE: Esse comando encerra a sessão e remove os executores\n",
    "# Sempre lembre de executar isso ao terminar de utilizar o Spark para evitar desperdício de recursos\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d93920-ccdf-4495-a8bc-f9f3d3786eae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
